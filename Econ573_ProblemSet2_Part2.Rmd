---
title: "Econ 573: Problem Set 2 - Part 2"
output:
  pdf_document: default
  html_document: default
code_folding: show
---

```{r}
Auto = read.csv("C:/Users/mateo/OneDrive - University of North Carolina at Chapel Hill/Courses/Spring 2025/Econ 573/Auto.csv")
```

*Question 8*

*(a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.*

```{r}
Auto$horsepower <- as.numeric(as.character(Auto$horsepower)) #Convert the horsepower variable form categorical to numeric
lmmpg = lm(mpg~horsepower, data = Auto) #Perform the simple linear regression
summary(lmmpg)
```

There seems to be a small negative relationship between the amount of miles per gallon for a car and its horsepower. From its p-value, which is very close to $0$, we can infer strong evidence for this relationship such that it is statistically significant. The predicted coefficient for horsepower,$\beta_1 = -0.157845$, tells us that every one-unit increase in horsepower decreases mpg by -0.157845.

```{r}
horsepower98 = data.frame(horsepower = 98)
predict(lmmpg, horsepower98, interval = "confidence") #Average mpg for cars with 98 horsepower
predict(lmmpg, horsepower98, interval =  "prediction") #Predicted mpg for a randomly selected car with 98 horsepower
```

From these results we can interpret that we are 95% confident that the mean mpg for cars with 98 horsepower is between 23.80 and 25.40. For an individual car with 98 horsepower, we predict with 95% confidence that its mpg will be between 20.50 and 28.34.

*Question 9*

*(a) Produce a scatterplot matrix which includes all of the variables in the data set.*

```{r}
pairs(Auto[, sapply(Auto, is.numeric)]) #Creates a scatterplot matrix for all the variables, selecting only numeric columns
```

*(b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, cor() which is qualitative.*

```{r}
cor(Auto[, sapply(Auto, is.numeric)]) #Creates the matrix of correlations between variables, excluding the name variable.
```

*(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output.*

```{r}
mlmmpg = lm(mpg~cylinders + displacement + horsepower + weight + acceleration + year + origin, data = Auto)
summary(mlmmpg)
```

The weight, year, and origin of the car appear to be the most statistically significant predictors, with p-values close to 0, indicating a strong relationship with mpg. The displacement predictor also seems statistically significant with a p-value close to 0.001. The remaining predictors have p-values closer to 1, suggesting they lack statistical significance in explaining variations in mpg.

The origin, year, and displacement predictors show a positive relationship with mpg. Specifically, origin has the strongest effect on mpg, with a coefficient of 1.4261 for one category relative to the baseline, followed by year with an effect of 0.7508 for each unit increase. Displacement has a smaller positive effect, with a coefficient of 0.0199 per unit increase. Conversely, weight exhibits a negative effect, reducing mpg by 0.0065 for every unit increase in weight.

*(d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?*

```{r}
par(mfrow = c(2,2)) #Set plot layout to 2x2 grid
plot(mlmmpg) #Generate plot
```

The diagnostic plots reveal several insights about the model fit. The Q-Q plot shows that the residuals are approximately normally distributed, with minor deviations at the tails. The Residuals vs. Fitted plot suggests a slight non-linearity in the relationship between the predictors and mpg. The Scale-Location plot indicates mild heteroscedasticity, with residual variability increasing slightly as fitted values increase. Finally, the Residuals vs. Leverage plot does not highlight any high-leverage or overly influential observations, suggesting that no single data point disproportionately affects the model.

*(e) Use the* \* *and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?*

We can run a linear regression model incorporating interaction terms between the most statistically significant predictors to assess whether these interactions provide additional explanatory power for understanding the variability in miles per gallon (mpg).

```{r}
mpg_interac = lm(mpg~ cylinders + displacement + horsepower + weight + acceleration + year + origin + weight*year + weight*origin + weight*displacement + year*origin + year*displacement + origin* displacement + horsepower*year + horsepower*origin + horsepower*weight+ horsepower*displacement, data = Auto)
summary(mpg_interac)
```

From the regression results, we observe that most interaction coefficients are not only small but also statistically insignificant. However, there are a few exceptions. With a p-value close to 0, the interaction terms weight:displacement and horsepower:year provide strong evidence of statistical significance, with an effect of 1.814e−5 and 6.93e-05, respectively, on mpg. The interaction terms horsepower:origin and displacement:origin also appear to be somewhat significant, with the former having a p-value close to 0.001 and the latter close to 0.01.

Additionally, we observe that the inclusion of interaction terms causes for the displacement, weight and origin predictors that were previously determined to be statistically significant to become insignificant. However, in the case of horsepower, strong evidence for its statistical significance was now found. This effect can be attributed to increased multicollinearity between the interaction terms and their associated individual predictors.

*(f) Try a few different transformations of the variables, such as log(X), √ X, X2. Comment on your findings.*

Logarithmic Transformation of Weight:

```{r}
log_lmmpg = lm(mpg ~ cylinders + displacement + horsepower + log(weight) + acceleration + year + origin, data = Auto)
summary(log_lmmpg)
```

```{r}
par(mfrow = c(2,2))
plot(log_lmmpg)
```

With the logarithmic transformation of the weight predictor, we don't see any changes in the significance of the predictors. However, we do see an increase in the Adjusted R-squared compared to the original model (0.8431 > 0.8182)indicating a better fit. Residual plots exhibit virtually no changes compared to the original model.

Square Root Transformation of Acceleration and Horsepower:

```{r}
sqrt_lmmpg = lm(mpg ~ cylinders + displacement + sqrt(horsepower) + weight + sqrt(acceleration) + year + origin, data = Auto)
summary(sqrt_lmmpg)
```
```{r}
par(mfrow = c(2,2))
plot(sqrt_lmmpg)
```

With the square root transformation applied to acceleration and horsepower, we observe that the transformed horsepower predictor gained statistical significance. Additionally, the Adjusted R-squared value increased compared to the original model (0.8243 > 0.8182). However, residual plots remain virtually unchanged from those of the original model.

Quadratic Transformation of Horsepower and Weight:

```{r}
quad_lmmpg = lm(mpg ~ cylinders + displacement + I(horsepower^2) + I(weight^2) + acceleration + year + origin, data = Auto)
summary(quad_lmmpg)
```
```{r}
par(mfrow = c(2,2))
plot(quad_lmmpg)
```

When applying a quadratic transformation to horsepower and weight, the acceleration predictor lost its statistical significance. Furthermore, the transformed model exhibited a lower Adjusted R-squared value compared to the original model (0.7858 < 0.8182) suggesting a worse overall fit. Residual plots also show no noticeable differences from the original model.

*Question 13*

```{r}
set.seed(1)
```

*(a) Using the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0, 1) distribution. This represents a feature, X. *

```{r}
x = rnorm(100, mean = 0, sd = 1)
```

*(b) Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a N(0, 0.25) distribution—a normal distribution with mean zero and variance 0.25. *

```{r}
eps = rnorm(100, mean = 0, sd = sqrt(0.25))
```

*(c) Using x and eps, generate a vector y according to the model Y = −1+0.5X + ϵ. (3.39) What is the length of the vector y? What are the values of β0 and β1 in this linear model?*

```{r}
y = -1 + 0.5*x + eps
```

The length of the vector will be 100, the same as both x and eps. The value of the intercept parameter is $\beta_0 = -1$, while the value of the slope parameter is $\beta_1 = 0.5$.

*(d) Create a scatterplot displaying the relationship between x and y. Comment on what you observe.*

```{r}
plot(x, y, main="Scatterplot of X vs Y", xlab="X", ylab="Y", pch=19, col="cadetblue1")
abline(-1, 0.5, col="black", lwd=2, lty=2)
```
The scatterplot shows a positive, approximately linear relationship between X and Y. As X increases, Y tends to increase as well. However, the random noise introduced by the eps term creates some variability around the linear trend, resulting in a moderate spread of points across the regression line.

*(e) Fit a least squares linear model to predict y using x. Comment on the model obtained. How do βˆ0 and βˆ1 compare to β0 and β1?*

```{r}
lm = lm(y~x)
summary(lm)
```
The estimated intercept $\hat{\beta}_0 = -0.98632$ is close to the true value $\beta_0 = -1$. Similarly, the estimated coefficient for x $\hat{\beta}_1 = 0.51058$ is close to the true value $\beta_1 = 0.5$. The small deviation of these estimated values from the true values is due to the random noise introduced by the error term, ϵ. However, we can conclude that the model accurately captures the true relationship between x and y. 

Additionally, there is strong evidence that x is a statistically significant predictor, as indicated by a p-value of $2*10^{-16}$. However, the R-squared value for the model is very low at 0.4346, or the model is relatively low at 0.4346, meaning that only 43.46% of the variability in Y is explained by the predictor variable, x, in this linear regression model.

*Question 15*

```{r}
Boston <- read.csv("C:/Users/mateo/OneDrive - University of North Carolina at Chapel Hill/Courses/Spring 2025/Econ 573/ALL+CSV+FILES+-+2nd+Edition+-+corrected/ALL CSV FILES - 2nd Edition/Boston.csv", header = TRUE)
Boston <- Boston[, sapply(Boston, is.numeric)]
Boston <- Boston[, sapply(Boston, function(col) length(unique(col)) > 1)]
Boston <- Boston[, !names(Boston) %in% "X"]
Boston$chas <- as.factor(Boston$chas)
```

*(a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.*

```{r}
model_zn = lm(crim ~ zn, data = Boston)
model_chas = lm(crim ~ chas, data = Boston)
model_indus = lm(crim ~ indus, data = Boston)
model_nox = lm(crim ~ nox, data = Boston)
model_rm = lm(crim ~ rm, data = Boston)
model_age = lm(crim ~ age, data = Boston)
model_dis = lm(crim ~ dis, data = Boston)
model_rad = lm(crim ~ rad, data = Boston)
model_tax = lm(crim ~ tax, data = Boston)
model_ptratio = lm(crim ~ ptratio, data = Boston)
model_lstat = lm(crim ~ lstat, data = Boston)
model_medv = lm(crim ~ medv, data = Boston)

summary(model_zn)
summary(model_chas)
summary(model_indus)
summary(model_nox)
summary(model_rm)
summary(model_age)
summary(model_dis)
summary(model_rad)
summary(model_tax)
summary(model_ptratio)
summary(model_lstat)
summary(model_medv)

par(mfrow = c(3, 4))

plot(Boston$zn, Boston$crim, main = "zn vs crim", xlab = "zn", ylab = "Crime Rate")
abline(model_zn, col = "red", lwd = 2)

plot(Boston$indus, Boston$crim, main = "indus vs crim", xlab = "indus", ylab = "Crime Rate")
abline(model_indus, col = "red", lwd = 2)

plot(Boston$nox, Boston$crim, main = "nox vs crim", xlab = "nox", ylab = "Crime Rate")
abline(model_nox, col = "red", lwd = 2)

plot(Boston$rm, Boston$crim, main = "rm vs crim", xlab = "rm", ylab = "Crime Rate")
abline(model_rm, col = "red", lwd = 2)

plot(Boston$age, Boston$crim, main = "age vs crim", xlab = "age", ylab = "Crime Rate")
abline(model_age, col = "red", lwd = 2)

plot(Boston$dis, Boston$crim, main = "dis vs crim", xlab = "dis", ylab = "Crime Rate")
abline(model_dis, col = "red", lwd = 2)

plot(Boston$rad, Boston$crim, main = "rad vs crim", xlab = "rad", ylab = "Crime Rate")
abline(model_rad, col = "red", lwd = 2)

plot(Boston$tax, Boston$crim, main = "tax vs crim", xlab = "tax", ylab = "Crime Rate")
abline(model_tax, col = "red", lwd = 2)

plot(Boston$ptratio, Boston$crim, main = "ptratio vs crim", xlab = "ptratio", ylab = "Crime Rate")
abline(model_ptratio, col = "red", lwd = 2)

plot(Boston$lstat, Boston$crim, main = "lstat vs crim", xlab = "lstat", ylab = "Crime Rate")
abline(model_lstat, col = "red", lwd = 2)

plot(Boston$medv, Boston$crim, main = "medv vs crim", xlab = "medv", ylab = "Crime Rate")
abline(model_medv, col = "red", lwd = 2)

par(mfrow = c(1, 1))
```
In all of the simple linear regression models, we observe strong evidence of statistical significance for each predictor, with p-values close to 0. This suggests that, when considered individually, each predictor has a strong association with per capita crime rate (crim).

However, this is expected because each model only includes one predictor at a time, meaning the regression is not accounting for interactions or correlations between multiple predictors. Some of these relationships may weaken when we include multiple predictors in a multiple regression model, due to multicollinearity or shared explanatory power.

*(b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0? *

```{r}
multi_model <- lm(crim ~ ., data = Boston)
summary(multi_model)
```
In the multiple regression model, the number of statistically significant predictors is drastically reduced compared to the simple regression models. In this case, only dis (distance to employment centers), rad (accessibility to highways), and medv (median home value) remain statistically significant, with p-values close to 0.

This suggests that we can confidently reject the null hypothesis, $H_0 : \beta_j = 0$ for these predictors at all conventional confidence levels.

*(c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.*

```{r}
simple_coefficients <- sapply(names(Boston)[-1], function(var) {
  model <- lm(as.formula(paste("crim ~", var)), data = Boston)  # Fit model
  coef(model)[2]  # Extract the coefficient (slope)
})
multi_coefficients <- coef(multi_model)[-1] 
plot(simple_coefficients, multi_coefficients, 
     xlab = "Simple Reg. Coeff.", 
     ylab = "Multi. Reg. Coeff.", 
     main = "Simple vs Multiple Reg. Coefficients", 
     pch = 19, col = "blue")

abline(a = 0, b = 1, col = "red", lwd = 2) 

coefficient_differences <- simple_coefficients - multi_coefficients
sorted_differences <- sort(coefficient_differences, decreasing = TRUE)
print(sorted_differences)

largest_changes <- sort(abs(coefficient_differences), decreasing = TRUE)
print(largest_changes)
```
The comparison between simple and multiple regression coefficients reveals that predictors like nox and rm experience substantial changes, with their coefficients drastically decreasing in the multiple regression model. This suggests these predictors are highly correlated with others and their effects are diminished when accounting for additional variables. In contrast, predictors like lstat, ptratio, and age remain consistent across both models, indicating they are strong independent predictors of crim. 

*(d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, ft a model of the form Y = β0 + β1X + β2X2 + β3X3 + ϵ.*

```{r}
# Fit polynomial regression models for each predictor
model_zn <- lm(crim ~ poly(zn, 3), data = Boston)
model_indus <- lm(crim ~ poly(indus, 3), data = Boston)
model_nox <- lm(crim ~ poly(nox, 3), data = Boston)
model_rm <- lm(crim ~ poly(rm, 3), data = Boston)
model_age <- lm(crim ~ poly(age, 3), data = Boston)
model_dis <- lm(crim ~ poly(dis, 3), data = Boston)
model_rad <- lm(crim ~ poly(rad, 3), data = Boston)
model_tax <- lm(crim ~ poly(tax, 3), data = Boston)
model_ptratio <- lm(crim ~ poly(ptratio, 3), data = Boston)
model_lstat <- lm(crim ~ poly(lstat, 3), data = Boston)
model_medv <- lm(crim ~ poly(medv, 3), data = Boston)

summary(model_zn)
summary(model_indus)
summary(model_nox)
summary(model_rm)
summary(model_age)
summary(model_dis)
summary(model_rad)
summary(model_tax)
summary(model_ptratio)
summary(model_lstat)
summary(model_medv)
```
The results reveal strong evidence of non-linear relationships between several predictors and crime rate (crim). Notably, nox (nitric oxide concentration), dis (distance to employment centers), and medv (median home value) exhibit highly significant quadratic and cubic terms, with p-values close to 0. This indicates complex non-linear associations that  impact crime rate. Predictors like indus (non-retail business acres), age (proportion of older housing), and ptratio (pupil-teacher ratio) show moderate non-linear effects with significant quadratic and cubic terms with their p-values being less than 0.01

In contrast, predictors such as zn (residential land zoning), rad (accessibility to highways), and tax (property tax rate) demonstrate weak or no significant non-linear effects, suggesting simpler models may suffice for them. In conclusion, we could incorporate non-linear terms for key predictors like nox, dis, and medv to enhance our model.
